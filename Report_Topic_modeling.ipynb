{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBXeYmSzm5jRX0J/c3D6Lo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niangmag/NLP-Assignment-3/blob/Topic_Modeling/Report_Topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH6PZBCNfqAh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report on Topic Modeling (LDA and NMF)\n",
        "\n",
        "This report presents the results of applying two topic modeling techniques, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), to a dataset of newsgroups. The objective is to identify the main themes present in the documents.\n",
        "\n",
        "1. Data Preprocessing\n",
        "\n",
        "Before applying the models, the text data was preprocessed. This included:\n",
        "\n",
        "Lowercasing the text.\n",
        "Removing non-alphabetic characters.\n",
        "Tokenization (splitting into words).\n",
        "Removing stopwords.\n",
        "Lemmatization (reducing words to their base form).\n",
        "This cleaning process is essential to reduce noise in the data and improve the quality of the extracted topics.\n",
        "\n",
        "2. Vectorization\n",
        "\n",
        "The cleaned text data was converted into numerical representations using two vectorization techniques:\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency): Used for NMF. This method weights words based on their frequency in a document relative to their frequency across the entire corpus. This helps highlight words that are important to a specific document but not too common overall.\n",
        "Count Vectorizer: Used for LDA. This method simply counts the frequency of each word in each document.\n",
        "Both vectorizers were configured to consider a maximum of 1000 features (words) and ignore words that appear in more than 95% of documents or fewer than 2 documents.\n",
        "\n",
        "3. Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "Model: An LDA model with 10 components (topics) was trained on the Count Vectorizer data.\n",
        "Outputs (Keywords per Topic): The analysis of keywords for each topic (as displayed in your LDA Topics: output) reveals distinct themes. For example:\n",
        "Topic 4 seems related to sports, specifically hockey (game, team, year, player, play, win, hockey).\n",
        "Topic 3 could be about computer hardware (drive, disk, scsi, controller, system, hard).\n",
        "Topic 1 seems related to health or medical problems (problem, doctor, patient).\n",
        "Performance: The interpretation of the topics generated by LDA appears consistent with the types of discussions that might be found in various newsgroups. The quality of the topics will depend on the relevance of the identified keywords.\n",
        "4. Non-negative Matrix Factorization (NMF)\n",
        "\n",
        "Model: An NMF model with 10 components (topics) was trained on the TF-IDF data.\n",
        "Outputs (Keywords per Topic): The topics generated by NMF (as displayed in your NMF Topics: output) also show interesting themes, sometimes similar to those from LDA but with nuances due to the vectorization method (TF-IDF). For example:\n",
        "Topic 2 is clearly related to sports, particularly hockey (game, team, player, play, hockey).\n",
        "Topic 3 is also about computer hardware (drive, scsi, disk, hard, floppy).\n",
        "Topic 4 seems to be about cars (car, driver, engine, price).\n",
        "Performance: NMF, using TF-IDF, tends to highlight words that are more discriminative for the topics. The topics also appear well-defined and interpretable.\n",
        "5. Visualization\n",
        "\n",
        "The visualizations (bar charts and word clouds) are valuable tools for understanding and comparing the topics extracted by LDA and NMF.\n",
        "\n",
        "Bar Charts: They show the most important words for each topic and their respective weights within the topic. This allows for a direct comparison of the importance of words within each theme.\n",
        "Word Clouds: They provide a quick visual representation of the relative importance of words within each topic, where the size of the word indicates its weight.\n",
        "These visualizations visually confirm the themes identified by inspecting the keywords and help to better understand the composition of each topic.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Both models, LDA and NMF, successfully extracted meaningful topics from the newsgroup dataset. The topics identified by the two methods show similarities, reflecting the dominant themes in the data. The choice between LDA and NMF can depend on the specific nature of the data and the objectives of the analysis. LDA is based on a probabilistic model and assumes that each document is a mixture of topics, while NMF is an algebraic approach that decomposes the document-term matrix. In this case, both provided interpretable results, and the visualizations greatly aided in understanding the topics."
      ],
      "metadata": {
        "id": "6_yldwmHfqpe"
      }
    }
  ]
}